{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym  # Use gymnasium instead of gym\n",
    "from gymnasium.spaces import Discrete, Box  # Use gymnasium spaces\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import SAC\n",
    "from gymnasium import spaces\n",
    "import re\n",
    "import csv\n",
    "# Entorno personalizado para el trading de acciones\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, initial_balance=10000, shares_per_step=10, commission=0.001, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.df = df  # DataFrame con los datos del mercado\n",
    "        self.initial_balance = initial_balance  # Balance inicial\n",
    "        self.balance = initial_balance  # Balance actual\n",
    "        self.net_worth = initial_balance  # Patrimonio neto actual\n",
    "        self.shares_held = 0  # Cantidad de acciones en posesión\n",
    "        self.shares_per_step = shares_per_step  # Cantidad de acciones a comprar/vender en cada paso\n",
    "        self.commission = commission  # Comisión por transacción\n",
    "        self.current_step = 0  # Paso actual en el entorno\n",
    "        self.reward_range = (-float('inf'), float('inf'))  # Rango de recompensas\n",
    "        self.action_space = Discrete(3)  # Espacio de acciones: 0: hold, 1: buy, 2: sell\n",
    "        self.observation_space = Box(low=0, high=1, shape=(5,), dtype=np.float32)  # Espacio de observaciones\n",
    "        self.render_mode = render_mode  # Modo de renderización\n",
    "        self.action_history = []  # Historial de acciones\n",
    "        self.observation_space = Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    # Función para generar la siguiente observación\n",
    "    def _next_observation(self):\n",
    "        frame = np.array([\n",
    "            self.df.iloc[self.current_step]['Close'] / 1000,  # Precio de cierre escalado\n",
    "            self.df.iloc[self.current_step]['Volume'] / 1000000,  # Volumen escalado\n",
    "            self.balance / self.initial_balance,  # Balance relativo al balance inicial\n",
    "            self.shares_held / 100,  # Acciones en posesión escaladas\n",
    "            self.net_worth / self.initial_balance,  # Patrimonio neto relativo al balance inicial\n",
    "        ], dtype=np.float32)\n",
    "        return frame\n",
    "\n",
    "    # Función para realizar una acción\n",
    "    def _take_action(self, action):\n",
    "        current_price = self.df.iloc[self.current_step]['Close']  # Precio actual\n",
    "        trade_quantity = self.shares_per_step  # Cantidad a transar\n",
    "        cost = trade_quantity * current_price * (1 + self.commission)  # Costo de la transacción\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.balance >= cost:  # Verificar si hay suficiente balance\n",
    "                self.balance -= cost  # Reducir el balance\n",
    "                self.shares_held += trade_quantity  # Aumentar las acciones en posesión\n",
    "        elif action == 2:  # Vender\n",
    "            if self.shares_held >= trade_quantity:  # Verificar si hay suficientes acciones\n",
    "                self.balance += trade_quantity * current_price * (1 - self.commission)  # Aumentar el balance\n",
    "                self.shares_held -= trade_quantity  # Reducir las acciones en posesión\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price  # Calcular el patrimonio neto\n",
    "\n",
    "    # Función para realizar un paso en el entorno\n",
    "    def step(self, action):\n",
    "        terminated = self.current_step >= len(self.df) - 1  # Verificar si el episodio ha terminado\n",
    "        truncated = False  # No se utiliza en este entorno\n",
    "\n",
    "        if not terminated:\n",
    "            self.current_step += 1  # Avanzar al siguiente paso\n",
    "            self._take_action(action)  # Realizar la acción\n",
    "            obs = self._next_observation()  # Obtener la siguiente observación\n",
    "            reward = (self.net_worth - self.initial_balance) / self.initial_balance  # Calcular la recompensa\n",
    "        else:\n",
    "            obs = self._next_observation()  # Obtener la observación final\n",
    "            reward = 0  # Recompensa cero al final del episodio\n",
    "\n",
    "        info = {'step': self.current_step, 'balance': self.balance, 'shares_held': self.shares_held, 'net_worth': self.net_worth}  # Información adicional\n",
    "        self.action_history.append([self.current_step, action, self.df.iloc[self.current_step]['Close']])  # Registrar la acción\n",
    "        return obs, reward, terminated, truncated, info  # Devolver los resultados\n",
    "\n",
    "    # Función para renderizar el entorno (opcional)\n",
    "    def render(self, mode='human'):\n",
    "        if self.render_mode is not None:\n",
    "            print(f'Step: {self.current_step}')\n",
    "            print(f'Balance: {self.balance}')\n",
    "            print(f'Shares held: {self.shares_held}')\n",
    "            print(f'Net worth: {self.net_worth}')\n",
    "\n",
    "    # Función para resetear el entorno\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.balance = self.initial_balance  # Resetear el balance\n",
    "        self.net_worth = self.initial_balance  # Resetear el patrimonio neto\n",
    "        self.shares_held = 0  # Resetear las acciones en posesión\n",
    "        self.current_step = 0  # Resetear el paso actual\n",
    "        obs = self._next_observation()  # Obtener la observación inicial\n",
    "        info = {}  # Información adicional\n",
    "        self.action_history = []  # Resetear el historial de acciones\n",
    "        return obs, info  # Devolver la observación y la información\n",
    "\n",
    "# Entorno personalizado para el trading de acciones compatible con SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym  # Use gymnasium instead of gym\n",
    "from gymnasium.spaces import Discrete, Box  # Use gymnasium spaces\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import SAC\n",
    "from gymnasium import spaces\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Entorno personalizado para el trading de acciones compatible con SAC\n",
    "class StockTradingEnvSAC(gym.Env):\n",
    "    def __init__(self, df, initial_balance=10000, shares_per_step=10, commission=0.001, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.df = df  # DataFrame con los datos del mercado\n",
    "        self.initial_balance = initial_balance  # Balance inicial\n",
    "        self.balance = initial_balance  # Balance actual\n",
    "        self.net_worth = initial_balance  # Patrimonio neto actual\n",
    "        self.shares_held = 0  # Cantidad de acciones en posesión\n",
    "        self.shares_per_step = shares_per_step  # Cantidad de acciones a comprar/vender en cada paso\n",
    "        self.commission = commission  # Comisión por transacción\n",
    "        self.current_step = 0  # Paso actual en el entorno\n",
    "        self.reward_range = (-float('inf'), float('inf'))  # Rango de recompensas\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)  # Espacio de acciones continuo\n",
    "        self.observation_space = Box(low=0, high=1, shape=(5,), dtype=np.float32)  # Espacio de observaciones\n",
    "        self.render_mode = render_mode  # Modo de renderización\n",
    "        self.action_history = []  # Historial de acciones\n",
    "        self.observation_space = Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    # Función para generar la siguiente observación\n",
    "    def _next_observation(self):\n",
    "        frame = np.array([\n",
    "            self.df.iloc[self.current_step]['Close'] / 1000,  # Precio de cierre escalado\n",
    "            self.df.iloc[self.current_step]['Volume'] / 1000000,  # Volumen escalado\n",
    "            self.balance / self.initial_balance,  # Balance relativo al balance inicial\n",
    "            self.shares_held / 100,  # Acciones en posesión escaladas\n",
    "            self.net_worth / self.initial_balance,  # Patrimonio neto relativo al balance inicial\n",
    "        ], dtype=np.float32)\n",
    "        return frame\n",
    "\n",
    "    # Función para realizar una acción\n",
    "    def _take_action(self, action):\n",
    "        current_price = self.df.iloc[self.current_step]['Close']  # Precio actual\n",
    "        trade_quantity = self.shares_per_step * action[0]  # Cantidad a transar, ajustada por la acción\n",
    "\n",
    "        cost = abs(trade_quantity) * current_price * (1 + self.commission)  # Costo de la transacción\n",
    "\n",
    "        if action[0] > 0:  # Comprar\n",
    "            if self.balance >= cost:  # Verificar si hay suficiente balance\n",
    "                self.balance -= cost  # Reducir el balance\n",
    "                self.shares_held += trade_quantity  # Aumentar las acciones en posesión\n",
    "        elif action[0] < 0:  # Vender\n",
    "            if self.shares_held >= abs(trade_quantity):  # Verificar si hay suficientes acciones\n",
    "                self.balance += abs(trade_quantity) * current_price * (1 - self.commission)  # Aumentar el balance\n",
    "                self.shares_held -= abs(trade_quantity)  # Reducir las acciones en posesión\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price  # Calcular el patrimonio neto\n",
    "\n",
    "    # Función para realizar un paso en el entorno\n",
    "    def step(self, action):\n",
    "        terminated = self.current_step >= len(self.df) - 1  # Verificar si el episodio ha terminado\n",
    "        truncated = False  # No se utiliza en este entorno\n",
    "\n",
    "        if not terminated:\n",
    "            self.current_step += 1  # Avanzar al siguiente paso\n",
    "            self._take_action(action)  # Realizar la acción\n",
    "            obs = self._next_observation()  # Obtener la siguiente observación\n",
    "            reward = (self.net_worth - self.initial_balance) / self.initial_balance  # Calcular la recompensa\n",
    "        else:\n",
    "            obs = self._next_observation()  # Obtener la observación final\n",
    "            reward = 0  # Recompensa cero al final del episodio\n",
    "\n",
    "        info = {'step': self.current_step, 'balance': self.balance, 'shares_held': self.shares_held, 'net_worth': self.net_worth}  # Información adicional\n",
    "        self.action_history.append([self.current_step, action, self.df.iloc[self.current_step]['Close']])  # Registrar la acción\n",
    "        return obs, reward, terminated, truncated, info  # Devolver los resultados\n",
    "\n",
    "    # Función para renderizar el entorno (opcional)\n",
    "    def render(self, mode='human'):\n",
    "        if self.render_mode is not None:\n",
    "            print(f'Step: {self.current_step}')\n",
    "            print(f'Balance: {self.balance}')\n",
    "            print(f'Shares held: {self.shares_held}')\n",
    "            print(f'Net worth: {self.net_worth}')\n",
    "\n",
    "    # Función para resetear el entorno\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.balance = self.initial_balance  # Resetear el balance\n",
    "        self.net_worth = self.initial_balance  # Resetear el patrimonio neto\n",
    "        self.shares_held = 0  # Resetear las acciones en posesión\n",
    "        self.current_step = 0  # Resetear el paso actual\n",
    "        obs = self._next_observation()  # Obtener la observación inicial\n",
    "        info = {}  # Información adicional\n",
    "        self.action_history = []  # Resetear el historial de acciones\n",
    "        return obs, info  # Devolver la observación y la información\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo_model(train_data_path, base_path):\n",
    "    \"\"\"\n",
    "    Entrena un modelo PPO con los datos de entrenamiento proporcionados.\n",
    "\n",
    "    Args:\n",
    "        train_data_path (str): Ruta al archivo CSV de datos de entrenamiento.\n",
    "        base_path (str): Ruta base del proyecto para guardar el CSV de resultados.\n",
    "    \"\"\"\n",
    "    # Cargar los datos de entrenamiento\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "\n",
    "    # Crear y verificar el entorno de entrenamiento\n",
    "    train_env = StockTradingEnv(train_df, render_mode=None)\n",
    "    check_env(train_env)\n",
    "\n",
    "    # Vectorizar el entorno de entrenamiento\n",
    "    vec_env = DummyVecEnv([lambda: train_env])\n",
    "\n",
    "  \n",
    "    # Definir rangos de hiperparámetros\n",
    "    learning_rates = [0.00007]  # Tasas de aprendizaje\n",
    "    gammas = [0.99]  # Factores de descuento\n",
    "    n_steps_list = [128]  # Número de pasos antes de actualizar el modelo\n",
    "    ent_coefs = [0.01]  # Coeficientes de la pérdida de entropía\n",
    "    vf_coefs = [0.5]  # Coeficientes de la pérdida de la función de valor\n",
    "    max_grad_norms = [0.5]  # Valores máximos para la normalización del gradiente\n",
    "    gae_lambdas = [0.95]  # GAE lambda parameter\n",
    "    batch_sizes = [128]  # Batch size\n",
    "\n",
    "    \n",
    "    # # Definir rangos de hiperparámetros para PPO\n",
    "    # learning_rates = [0.0001, 0.0003, 0.0007, 0.001]  # Tasas de aprendizaje\n",
    "    # gammas = [0.95, 0.97, 0.99]  # Factores de descuento\n",
    "    # n_steps_list = [64, 128, 256]  # Número de pasos antes de actualizar el modelo\n",
    "    # ent_coefs = [0.01, 0.02, 0.05]  # Coeficientes de la pérdida de entropía\n",
    "    # vf_coefs = [0.5, 0.7, 0.9]  # Coeficientes de la pérdida de la función de valor\n",
    "    # max_grad_norms = [0.5, 1.0, 1.5]  # Valores máximos para la normalización del gradiente\n",
    "    # gae_lambdas = [0.9, 0.95, 0.99]  # GAE lambda parameter\n",
    "    # batch_sizes = [64, 128, 256]  # Batch size\n",
    "\n",
    "    # Extract information from the training data path\n",
    "    train_data_filename = os.path.basename(train_data_path)\n",
    "    is_filtered = \"no_filtrado\" not in train_data_path\n",
    "    is_normalized = \"no_normalizado\" not in train_data_path\n",
    "\n",
    "    # CSV file path\n",
    "    csv_file = os.path.join(base_path, \"training_results.csv\")\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "    # Iterar a través de las combinaciones de hiperparámetros\n",
    "    for learning_rate in learning_rates:\n",
    "        for gamma in gammas:\n",
    "            for n_steps in n_steps_list:\n",
    "                for ent_coef in ent_coefs:\n",
    "                    for vf_coef in vf_coefs:\n",
    "                        for max_grad_norm in max_grad_norms:\n",
    "                            for gae_lambda in gae_lambdas:\n",
    "                                for batch_size in batch_sizes:\n",
    "                                    # Definir el nombre del modelo\n",
    "                                    model_name = f\"ppo_lr{learning_rate}_gamma{gamma}_nsteps{n_steps}_ent{ent_coef}_vf{vf_coef}_gradnorm{max_grad_norm}_gae{gae_lambda}_batch{batch_size}\"\n",
    "\n",
    "                                    # Crear la carpeta para los entrenamientos\n",
    "                                    training_folder = os.path.join(os.path.dirname(train_data_path), \"entrenamientos\", \"PPO\")\n",
    "                                    os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "                                    # Crear la carpeta para el modelo entrenado\n",
    "                                    model_folder = os.path.join(training_folder, model_name)\n",
    "                                    os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "                                    model_path = os.path.join(model_folder, \"model\")\n",
    "\n",
    "                                    print(f\"Entrenando modelo: {model_name}\")\n",
    "\n",
    "                                    # Entrenar el modelo\n",
    "                                    model = PPO('MlpPolicy', vec_env, learning_rate=learning_rate, gamma=gamma, n_steps=n_steps,\n",
    "                                                ent_coef=ent_coef, vf_coef=vf_coef, max_grad_norm=max_grad_norm, gae_lambda=gae_lambda,\n",
    "                                                batch_size=batch_size, verbose=0, device='cpu')\n",
    "                                    # # Entrenar el modelo\n",
    "                                    # model = PPO('MlpPolicy',vec_env,verbose=0, device='cpu')\n",
    "                                    model.learn(total_timesteps=10000)\n",
    "\n",
    "                                    # Guardar el modelo\n",
    "                                    model.save(model_path)\n",
    "                                    print(f\"Modelo guardado en: {model_path}\")\n",
    "\n",
    "                                    # Guardar las estadísticas de entrenamiento\n",
    "                                    stats_path = os.path.join(model_folder, \"stats.txt\")\n",
    "                                    with open(stats_path, \"w\") as f:\n",
    "                                        f.write(f\"Tasa de Aprendizaje: {learning_rate}\\n\")\n",
    "                                        f.write(f\"Gamma: {gamma}\\n\")\n",
    "                                        f.write(f\"N Pasos: {n_steps}\\n\")\n",
    "                                        f.write(f\"Ent Coef: {ent_coef}\\n\")\n",
    "                                        f.write(f\"VF Coef: {vf_coef}\\n\")\n",
    "                                        f.write(f\"Max Grad Norm: {max_grad_norm}\\n\")\n",
    "                                        f.write(f\"GAE Lambda: {gae_lambda}\\n\")\n",
    "                                        f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "                                    print(f\"Estadísticas de entrenamiento guardadas en: {stats_path}\")\n",
    "\n",
    "                                    # Extract algorithm name from the model path\n",
    "                                    algorithm_name = os.path.basename(os.path.dirname(os.path.dirname(model_path)))\n",
    "\n",
    "                                    # Use a more robust regex to extract parameters\n",
    "                                    params = re.findall(r\"([a-z]+)([0-9\\.e-]+)\", model_name)\n",
    "                                    params_dict = {p[0]: p[1] for p in params}\n",
    "\n",
    "                                    # Write to CSV\n",
    "                                    with open(csv_file, mode='a', newline='') as f:\n",
    "                                        writer = csv.writer(f)\n",
    "                                        if not file_exists:\n",
    "                                            writer.writerow([\n",
    "                                                \"Algorithm\", \"Learning Rate\", \"Gamma\", \"N Steps\", \"Ent Coef\", \"VF Coef\", \"Max Grad Norm\", \"GAE Lambda\", \"Batch Size\",\n",
    "                                                \"Train Data\", \"Filtered\", \"Normalized\"\n",
    "                                            ])\n",
    "                                            file_exists = True  # Ensure header is only written once\n",
    "\n",
    "                                        writer.writerow([\n",
    "                                            algorithm_name, params_dict.get(\"lr\", \"\"), params_dict.get(\"gamma\", \"\"), params_dict.get(\"nsteps\", \"\"),\n",
    "                                            params_dict.get(\"ent\", \"\"), params_dict.get(\"vf\", \"\"), params_dict.get(\"gradnorm\", \"\"), params_dict.get(\"gae\", \"\"), params_dict.get(\"batch\", \"\"),\n",
    "                                            train_data_filename, is_filtered, is_normalized\n",
    "                                        ])\n",
    "\n",
    "    print(f\"Estadísticas de entrenamiento guardadas en: {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
